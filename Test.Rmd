---
title: "Engagingness in Music: a Computational Analysis"
output:
  flexdashboard::flex_dashboard:
    orientation: rows 
    theme:
      version: 4
      bootswatch: superhero
    css: stylesheet.css
---

# Placing Tracks in Context

In this portfolio I will examine how AI generated dance music compares to that of humans using a data set of both AI generated and human made tracks from the course computational musicology. The tracks are part of a collection (corpus) of music which is either composed by students of computational musicology, generated by AI or existing royalty free music. The features in the table below, just like their assigned values, were retrieved from essentia, an open-source C++ library for audio analysis and audio-based music information retrieval. All the tracks in the table have been analysed by this program which gave these results. The second table is a dataset which is filtered on if i considered the songs to be EDM, in order for me to copmpare only electorning dance tracks. Here is an explanation for what all the features mean: 

* ***Approachability*** reflects how pleasant and easy a song is to listen to, 

* ***Arousal*** measures its energy level, with higher values indicating more intensity.

* ***Danceability*** assesses how well a track is suited for dancing, based on rhythm, beat strength, and tempo. 

* ***Tempo*** is a feature which indicates the speed of the song, measured in beats per minute (BPM).

* ***Engagingness*** shows how likely a track is to hold the listener's attention.

* ***Instrumentalness*** estimates the presence of vocals, with higher values suggesting more instrumental content.

* ***Valence*** describes the overall mood of the song, where higher values correspond to more positive and cheerful tones, while lower values indicate a more subdued or serious sound.

These features together provide a clear overview of each track's musical profile, making it easier to analyze and compare songs.

```{r table, echo=FALSE}
library(knitr)
compmus <- read.csv('compmus2025_classcorpus.csv')
compmus_dance <- read.csv('selected_dance_tracks.csv')
kable(head(compmus, 5))
kable(head(compmus_dance, 5))
```

**Information on my submitted tracks**

**Hidde-s-1**:

I produced this song myself. I make music with clubs or festivals in mind as I like to DJ. For this track I tried to combine a mainstream house music sound and combine it with some more raw electronic sounds. 

**Hidde-s-2**:

This is a track I generated with Suno. I asked chat gpt what the key characteristics of a dance track in a sweaty club in Amsterdam were: “Punchy four-on-the-floor kick, deep rolling bass, crisp shuffled hi-hats, sharp claps, detuned wide synth leads, tension-filled breakdown, rising FX, massive sidechained drop, high-energy, club-focused groove.”

##

```{r plot-example, fig.width=14, fig.height=6}
library(ggplot2)
library(plotly)
library(dplyr)
black <- theme(
    plot.background = element_rect(fill = "black", color = NA),
    panel.background = element_rect(fill = "black", color = NA),
    legend.background = element_rect(fill = "black", color = NA),
    legend.key = element_rect(fill = "black"),
    plot.title = element_text(color = "white", face = "bold"),
    axis.title = element_text(color = "grey66"),
    axis.text = element_text(color = "grey66"),
    legend.text = element_text(color = "grey66"),
    legend.title = element_text(color = "grey66"),
    panel.grid.major = element_line(color = "#B3B3B3", linewidth = 0.5),
    panel.grid.minor = element_line(color = "#CCCCCC", linewidth = 0.3) 
)

# Select relevant columns
dance <- compmus %>%
  select(filename, danceability, engagingness, tempo)

# Identify specific data points to highlight by filename
my_song <- dance %>%
  filter(filename == "hidde-s-1")

ai_song <- dance %>%
  filter(filename == "hidde-s-2")
# Create ggplot
dance_plot <- ggplot(dance, aes(x = danceability, y = engagingness, color = tempo)) +
  geom_point(size = 3, alpha = 0.9) +  # Main points
  geom_smooth(method = "lm", color = "red", linetype = "dashed") +  # Trend line
  geom_point(data = my_song, aes(x = danceability, y = engagingness), 
             color = "cyan", size = 4, alpha = 1, shape = 21) +  # Highlighted point
  geom_text(data = my_song, aes(x = danceability, y = engagingness - 0.1, label = "My own song"), 
            color = "white", vjust = -1, size = 4) +  # Label
  scale_color_viridis_c(option = "plasma", name = "Tempo (BPM)") +
  geom_segment(data = my_song,
               aes(x = danceability, y = engagingness - 0.1,
                   xend = danceability, yend = engagingness),
               arrow = arrow(length = unit(0.2, "cm")),
               color = "red") +
  geom_point(data = ai_song, aes(x = danceability, y = engagingness), 
             color = "cyan", size = 4, alpha = 1, shape = 21) +  # Highlighted point
  geom_text(data = ai_song, aes(x = danceability, y = engagingness - 0.1, label = "AI generated song"), 
            color = "white", vjust = -1, size = 4) +  # Label
  scale_color_viridis_c(option = "plasma", name = "Tempo (BPM)") +
  geom_segment(data = ai_song,
               aes(x = danceability, y = engagingness - 0.1,
                   xend = danceability, yend = engagingness),
               arrow = arrow(length = unit(0.2, "cm")),
               color = "red") +
  
  labs(title = "Danceability vs. Engagingness",
       x = "Danceability",
       y = "Engagingness") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "right") +
  black

# Convert to interactive plot
ggplotly(dance_plot)

```

###

**My tracks in the class corpus**

This is a graph which has mapped the engagingness of each song compared to its danceability. The colour scale is based on the tempo of each song. The first noticable aspect of the graph is the seemingly positive correlation between danceability and engagingness which is shown by the red trend line. On average it is clear that in most cases a high danceability value means that same song will have a high engagingness rating aswell. From the colour scaling it can also be noticed that most songs that have high scores for those features also have a higher tempo. This could mean that those features are highly correlated or that the way essentia measured these features is similar in terms of computational analysis. It would be interesting to look at why this correlation seems to be in place, for instance through examining the roll of instrumentallness, or genre in combination with this analysis. 

The two points that are highlighted are a song I arranged by myself and one I generated with suno. What can be seen with these songs is that my own song performs higher in both danceability and engagingness than the AI song while they are the same genre and made with the same intention. We can't conclude alot yet just from this example, however it lead us to the hypothesis: Ai generated music is distinguishable from human made music. Which we are going to evaluate in the next tabs. 

# AI and Human generated music are indistuingishable through clustering

