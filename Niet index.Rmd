---
title: "Engagingness in Music: a Computational Analysis"
date: "2025-02-19"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: 
      version: 4
      bootswatch: superhero
css: stylesheet.css
---
```{r setup, include=FALSE}
library(flexdashboard)
library(plotly)
library(ggplot2)
library(dplyr)
library(DT)
library(here)
```

### Tempi

```{r tempi}
library(tidyverse)
source("compmus-11.R")
"features/hidde-s-1.json" |>
  compmus_energy_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Time (s)", y = "Energy Novelty")

"features/hidde-s-1.json" |>
  compmus_tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

### Chordogram
```{r chordogram}
library(tidyverse)
source("compmus-11.R")

major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

"features/hidde-s-1.json" |> 
  compmus_chroma(norm = "identity") |> 
  compmus_match_pitch_templates(
    chord_templates,         # Change to chord_templates if desired
    norm = "euclidean",       # Try different norms (and match it with what you used in `compmus_chroma`)
    distance = "cosine"   # Try different distance metrics
  ) |>
  ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(x = "Time (s)", y = "Template", fill = NULL) +
  theme_classic() 
```

***

This is a chordogram that represents my track hidde-s-1. It shows the corresponding chords that the model has analysed from the chromagram. We can see that the track is prominently in c (correct) but it can't figure out if it is in C major or Minor. This could be because the track consists basically of only the c note which means the model can't distinguish the mode any further than that. Further more the model seems to struggle in the chorus. This is to be expected as the chroma gram in this time frame is messy as well and this visualisation is based on that (already a bit messy) chroma gram. 

### Chromagrams
```{r}
library(tidyverse)
source("compmus-11.R")
library(rjson)

"features/hidde-s-1.json" |>                           # Change the track
  compmus_chroma(norm = "manhattan") |>                 # Change the norm
  ggplot(aes(x = time, y = pc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:11,
    minor_breaks = NULL,
    labels = c(
                "C", "C#|Db", "D", "D#|Eb",
                "E", "F", "F#|Gb", "G",
                "G#|Ab", "A", "A#|Bb", "B"
              )
  ) +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()     
```

***

This is a chromagram of my original track. Most of the track is C dominant which is correct. Interesting: from 50-80 seconds there's some higher notes, this corresponds to my song which drops at that point and starts playing harmonies. also from 150-170 there's a lot more c than normal, this is because there's a very high note playing a c as well as the rest of the track. 

### Cepstrograms
```{r}
library(tidyverse)
source("compmus-11.R")
library(rjson)

"features/hidde-s-1.json" |>                           # Change the track
  compmus_mfccs(norm = "manhattan") |>                  # Change the norm
  ggplot(aes(x = time, y = mfcc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:12,
    minor_breaks = NULL,
  ) +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(x = "Time (s)", y = "Coefficient Number", fill = NULL) +
  theme_classic()                 
```

### Chroma SSM
```{r}
  library(tidyverse)
  source("compmus-11.R")
  library(rjson)
  "features/hidde-s-1.json" |>                           # Change the track
    compmus_chroma(norm = "manhattan") |>                 # Change the norm
    compmus_self_similarity(
      feature = pc,
      distance = "aitchison"                             # Change the distance
    ) |>   
    ggplot(aes(x = xtime, y = ytime, fill = d)) + 
    geom_raster() +
    scale_fill_viridis_c(guide = "none") +               # Change the colours?
    labs(x = "Time (s)", y = NULL, fill = NULL) +
    theme_classic()                                      # Change the theme?
  
  
  "features/hidde-s-1.json" |>                           # Change the track
  compmus_mfccs(norm = "euclidean") |>                  # Change the norm
  compmus_self_similarity(
    feature = mfcc,
    distance = "angular"                             # Change the distance
  ) |>   
  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()   
```

***

The Chroma SSM shows that most of the song is quite similar in key and it shows that there are a lots of little moments of uniqueness in both of the chorus'. It could be that the software struggles with the organs in the track, which may be percieved as slightly offkey because they sound a bit detuned.

The timbre SSM is interesting as it cleary shows that when the breakdowns are happening. in the 20-40 and 125-150 there are breakdowns which are similar in timbre, since they have the same absence of drums.


### The Computational Musicology Dataset Explained 
<div id="first-tab-caption">
The following data set provides a detailed breakdown of key musical features for each track in it. The tracks are part of a collection (corpus) of music which is either composed by students of computational musicology, generated by AI or existing royalty free music. The features in the table below, just like their assigned values, were retrieved from essentia, an open-source C++ library for audio analysis and audio-based music information retrieval. All the tracks in the table have been analysed by this program which gave these results. Here is an explanation for what all the features mean: 

* ***Approachability*** reflects how pleasant and easy a song is to listen to, 

* ***Arousal*** measures its energy level, with higher values indicating more intensity.

* ***Danceability*** assesses how well a track is suited for dancing, based on rhythm, beat strength, and tempo. 

* ***Tempo*** is a feature which indicates the speed of the song, measured in beats per minute (BPM).

* ***Engagingness*** shows how likely a track is to hold the listener's attention.

* ***Instrumentalness*** estimates the presence of vocals, with higher values suggesting more instrumental content.

* ***Valence*** describes the overall mood of the song, where higher values correspond to more positive and cheerful tones, while lower values indicate a more subdued or serious sound.

These features together provide a clear overview of each track's musical profile, making it easier to analyze and compare songs.

***

```{r table, echo=FALSE}
library(knitr)
compmus <- read.csv('compmus2025-real.csv')
kable(compmus)
```


### Positive correlation between danceability and engagingness 

```{r plot-example}
library(ggplot2)
library(plotly)
library(dplyr)
black <- theme(
    plot.background = element_rect(fill = "black", color = NA),
    panel.background = element_rect(fill = "black", color = NA),
    legend.background = element_rect(fill = "black", color = NA),
    legend.key = element_rect(fill = "black"),
    plot.title = element_text(color = "white", face = "bold"),
    axis.title = element_text(color = "grey66"),
    axis.text = element_text(color = "grey66"),
    legend.text = element_text(color = "grey66"),
    legend.title = element_text(color = "grey66"),
    panel.grid.major = element_line(color = "#B3B3B3", linewidth = 0.5),
    panel.grid.minor = element_line(color = "#CCCCCC", linewidth = 0.3) 
)

# Select relevant columns
dance <- compmus %>%
  select(filename, danceability, engagingness, tempo)

# Identify specific data points to highlight by filename
my_song <- dance %>%
  filter(filename == "hidde-s-1")

ai_song <- dance %>%
  filter(filename == "hidde-s-2")
# Create ggplot
dance_plot <- ggplot(dance, aes(x = danceability, y = engagingness, color = tempo)) +
  geom_point(size = 3, alpha = 0.9) +  # Main points
  geom_smooth(method = "lm", color = "red", linetype = "dashed") +  # Trend line
  geom_point(data = my_song, aes(x = danceability, y = engagingness), 
             color = "cyan", size = 4, alpha = 1, shape = 21) +  # Highlighted point
  geom_text(data = my_song, aes(x = danceability, y = engagingness - 0.1, label = "My own song"), 
            color = "white", vjust = -1, size = 4) +  # Label
  scale_color_viridis_c(option = "plasma", name = "Tempo (BPM)") +
  geom_segment(data = my_song,
               aes(x = danceability, y = engagingness - 0.1,
                   xend = danceability, yend = engagingness),
               arrow = arrow(length = unit(0.2, "cm")),
               color = "red") +
  geom_point(data = ai_song, aes(x = danceability, y = engagingness), 
             color = "cyan", size = 4, alpha = 1, shape = 21) +  # Highlighted point
  geom_text(data = ai_song, aes(x = danceability, y = engagingness - 0.1, label = "AI generated song"), 
            color = "white", vjust = -1, size = 4) +  # Label
  scale_color_viridis_c(option = "plasma", name = "Tempo (BPM)") +
  geom_segment(data = ai_song,
               aes(x = danceability, y = engagingness - 0.1,
                   xend = danceability, yend = engagingness),
               arrow = arrow(length = unit(0.2, "cm")),
               color = "red") +
  
  labs(title = "Danceability vs. Engagingness",
       x = "Danceability",
       y = "Engagingness") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "right") +
  black

# Convert to interactive plot
ggplotly(dance_plot)

```

***

This is a graph which has mapped the engagingness of each song compared to its danceability. The colour scale is based on the tempo of each song. The first noticable aspect of the graph is the seemingly positive correlation between danceability and engagingness which is shown by the red trend line. On average it is clear that in most cases a high danceability value means that same song will have a high engagingness rating aswell. From the colour scaling it can also be noticed that most songs that have high scores for those features also have a higher tempo. This could mean that those features are highly correlated or that the way essentia measured these features is similar in terms of computational analysis. It would be interesting to look at why this correlation seems to be in place, for instance through examining the roll of instrumentallness, or genre in combination with this analysis. 

The two points that are highlighted are a song I arranged by myself and one I genreated with suno. What can be seen with these songs is that my own song performs higher in both danceability and engagingness than the ai song while they are the same genre and made with the same intention. Secondly the bpm on the AI track is acurately analysed while the one for my song is not, which could also be an interesting way of analysing the data: can ai analyse ai songs better than human made songs? 

### Information on my sumitted tracks

**Hidde-s-1**:

I produced this song myself. I make music with clubs or festivals in mind as I like to DJ. For this track I tried to combine a mainstream house music sound and combine it with some more raw electronic sounds. 

**Hidde-s-2**:

This is a track I generated with Suno. I asked chat gpt what the key characteristics of a dance track in a sweaty club in Amsterdam were:

“Punchy four-on-the-floor kick, deep rolling bass, crisp shuffled hi-hats, sharp claps, detuned wide synth leads, tension-filled breakdown, rising FX, massive sidechained drop, high-energy, club-focused groove.”

 