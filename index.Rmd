---
title: "How far is the AI producer compared to humans?"
output:
  flexdashboard::flex_dashboard:
    orientation: rows  
    theme:
      version: 4
      bootswatch: superhero
    css: stylesheet.css
---

# 1. Placing Tracks in Context

<div id="first-tab-caption">
In this portfolio I will examine how AI generated dance music compares to that of humans using a data set of both AI generated and human made tracks from the course computational musicology. The tracks are part of a collection (corpus) of music which is either composed by students of computational musicology, generated by AI or existing royalty free music. The features in the table below, just like their assigned values, were retrieved from essentia, an open-source C++ library for audio analysis and audio-based music information retrieval. All the tracks in the table have been analysed by this program which gave these results. The second table is a dataset which is filtered on if i considered the songs to be EDM, in order for me to copmpare only electorning dance tracks. Here is an explanation for what all the features mean: 

* ***Approachability*** reflects how pleasant and easy a song is to listen to, 

* ***Arousal*** measures its energy level, with higher values indicating more intensity.

* ***Danceability*** assesses how well a track is suited for dancing, based on rhythm, beat strength, and tempo. 

* ***Tempo*** is a feature which indicates the speed of the song, measured in beats per minute (BPM).

* ***Engagingness*** shows how likely a track is to hold the listener's attention.

* ***Instrumentalness*** estimates the presence of vocals, with higher values suggesting more instrumental content.

* ***Valence*** describes the overall mood of the song, where higher values correspond to more positive and cheerful tones, while lower values indicate a more subdued or serious sound.

These features together provide a clear overview of each track's musical profile, making it easier to analyze and compare songs.

```{r table, echo=FALSE}
library(knitr)
compmus <- read.csv('compmus2025_classcorpus.csv')
compmus_dance <- read.csv('selected_dance_tracks.csv')
kable(head(compmus, 5))
kable(head(compmus_dance, 5))
```

##

**Information on my submitted tracks**

**Hidde-s-1**:

I produced this song myself. I make music with clubs or festivals in mind as I like to DJ. For this track I tried to combine a mainstream house music sound and combine it with some more raw electronic sounds. 

**Hidde-s-2**:

This is a track I generated with Suno. I asked chat gpt what the key characteristics of a dance track in a sweaty club in Amsterdam were:

“Punchy four-on-the-floor kick, deep rolling bass, crisp shuffled hi-hats, sharp claps, detuned wide synth leads, tension-filled breakdown, rising FX, massive sidechained drop, high-energy, club-focused groove.”


```{r plot-example, fig.width=14, fig.height=6}
library(ggplot2)
library(plotly)
library(dplyr)
black <- theme(
    plot.background = element_rect(fill = "black", color = NA),
    panel.background = element_rect(fill = "black", color = NA),
    legend.background = element_rect(fill = "black", color = NA),
    legend.key = element_rect(fill = "black"),
    plot.title = element_text(color = "white", face = "bold"),
    axis.title = element_text(color = "grey66"),
    axis.text = element_text(color = "grey66"),
    legend.text = element_text(color = "grey66"),
    legend.title = element_text(color = "grey66"),
    panel.grid.major = element_line(color = "#B3B3B3", linewidth = 0.5),
    panel.grid.minor = element_line(color = "#CCCCCC", linewidth = 0.3) 
)

# Select relevant columns
dance <- compmus %>%
  select(filename, danceability, engagingness, tempo)

# Identify specific data points to highlight by filename
my_song <- dance %>%
  filter(filename == "hidde-s-1")

ai_song <- dance %>%
  filter(filename == "hidde-s-2")
# Create ggplot
dance_plot <- ggplot(dance, aes(x = danceability, y = engagingness, color = tempo)) +
  geom_point(size = 3, alpha = 0.9) +  # Main points
  geom_smooth(method = "lm", color = "red", linetype = "dashed") +  # Trend line
  geom_point(data = my_song, aes(x = danceability, y = engagingness), 
             color = "cyan", size = 4, alpha = 1, shape = 21) +  # Highlighted point
  geom_text(data = my_song, aes(x = danceability, y = engagingness - 0.1, label = "My own song"), 
            color = "white", vjust = -1, size = 4) +  # Label
  scale_color_viridis_c(option = "plasma", name = "Tempo (BPM)") +
  geom_segment(data = my_song,
               aes(x = danceability, y = engagingness - 0.1,
                   xend = danceability, yend = engagingness),
               arrow = arrow(length = unit(0.2, "cm")),
               color = "red") +
  geom_point(data = ai_song, aes(x = danceability, y = engagingness), 
             color = "cyan", size = 4, alpha = 1, shape = 21) +  # Highlighted point
  geom_text(data = ai_song, aes(x = danceability, y = engagingness - 0.1, label = "AI generated song"), 
            color = "white", vjust = -1, size = 4) +  # Label
  scale_color_viridis_c(option = "plasma", name = "Tempo (BPM)") +
  geom_segment(data = ai_song,
               aes(x = danceability, y = engagingness - 0.1,
                   xend = danceability, yend = engagingness),
               arrow = arrow(length = unit(0.2, "cm")),
               color = "red") +
  
  labs(title = "Danceability vs. Engagingness",
       x = "Danceability",
       y = "Engagingness") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "right") +
  black

# Convert to interactive plot
ggplotly(dance_plot)

```

**My tracks in the class corpus**

This is a graph which has mapped the engagingness of each song compared to its danceability. The colour scale is based on the tempo of each song. The first noticable aspect of the graph is the seemingly positive correlation between danceability and engagingness which is shown by the red trend line. On average it is clear that in most cases a high danceability value means that same song will have a high engagingness rating aswell. From the colour scaling it can also be noticed that most songs that have high scores for those features also have a higher tempo. This could mean that those features are highly correlated or that the way essentia measured these features is similar in terms of computational analysis. It would be interesting to look at why this correlation seems to be in place, for instance through examining the roll of instrumentallness, or genre in combination with this analysis. 

The two points that are highlighted are a song I arranged by myself and one I generated with suno. What can be seen with these songs is that my own song performs higher in both danceability and engagingness than the AI song while they are the same genre and made with the same intention. We can't conclude alot yet just from this example, however it lead us to the hypothesis: Ai generated music is distinguishable from human made music. Which we are going to evaluate in the next tabs. 


# 2. AI and Human generated music are indistuingishable through clustering

### {data-width=1500}
```{r cluster plot}
# Load libraries
library(tidyverse)
library(ggplot2)
library(cluster)
library(factoextra)
library(ggrepel)

cluster_colors <- c("1" = "#FF5C5C",  # hot coral red
                    "2" = "#00CFFF")

# Load dataset
compmus_dance <- read.csv("selected_dance_tracks.csv")

# Step 1: Select numeric features for clustering
features <- compmus_dance %>%
  select(approachability, arousal, danceability, engagingness,
         instrumentalness, tempo, valence)

# Step 2: Scale the data
scaled_features <- scale(features)

# Step 3: Perform k-means clustering
set.seed(42)
kmeans_result <- kmeans(scaled_features, centers = 2, nstart = 25)

# cluster info
compmus_dance$cluster <- as.factor(kmeans_result$cluster)

# true AI/human label
compmus_dance$label <- ifelse(compmus_dance$ai == TRUE, "AI", "Human")

# PCA for visualization
pca_result <- prcomp(scaled_features)

pca_df <- as.data.frame(pca_result$x[,1:2]) %>%
  mutate(cluster = compmus_dance$cluster,
         label = compmus_dance$label,
         id = compmus_dance$id)

# Plot the clusters and label ground truth
clusterplot <- ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster, shape = label, text=id)) +
  geom_point(size = 3, alpha = 0.8) +
  geom_text_repel(aes(label = id), size = 2.5, max.overlaps = 20) +
  scale_color_manual(values = cluster_colors) +
  theme_minimal(base_size = 14) +
  labs(title = "Clustering Dance Tracks: AI vs Human",
       x = "Principal Component 1",
       y = "Principal Component 2",
       color = "Cluster", shape = "True Label") +
  black

ggplotly(clusterplot, tooltip = "text")
```

### 

Clustering is an unsupervised machine learning technique that groups data points based on their similarity. In this case, we applied k-means clustering on various musical features such as arousal, danceability, instrumentalness, tempo, and valence, in order to see whether AI-generated songs can be distinguished from human-made tracks. The scatterplot above visualizes the clustering results after reducing dimensionality using Principal Component Analysis (PCA). Each point represents a track, colored by its assigned cluster and shaped by its actual label (AI or Human). What is noticeable from the graph is that that the model has found a decision boundry along the PCA axis', but the two groups do not overtly represent AI songs more than human made songs. We can thus see that there's is no distinguishable difference between the AI tracks and the human tracks at least not through clustering based on the dataset. 

The results suggest that AI and Human-generated dance tracks are not easily separable using unsupervised methods like clustering, which could indicate that AI music generation tools like Suno are good at learning and reproducing feature patterns found in human music or that the essentia features might not be sufficient to capture the stylistic differences.

The question remains of whether the distinction between "AI" and "Human" is becoming blurred and thus we must look further. 

# 3. AI might procuce less interesting melodies than humans

## chromagrams
```{r chromagrams-together, fig.width=14, fig.height=5}
library(tidyverse)
source("compmus_FV.R")
library(rjson)
library(gridExtra)

p1 <- "features/hidde-s-1.json" |>                           # Change the track
  compmus_chroma(norm = "manhattan") |>                 # Change the norm
  ggplot(aes(x = time, y = pc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:11,
    minor_breaks = NULL,
    labels = c(
                "C", "C#|Db", "D", "D#|Eb",
                "E", "F", "F#|Gb", "G",
                "G#|Ab", "A", "A#|Bb", "B"
              )
  ) +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title = "Chromagram of My Own Human Made Track", x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()   # your first chromagram
p2 <- "features/evan-l-2.json" |>                           # Change the track
  compmus_chroma(norm = "manhattan") |>                 # Change the norm
  ggplot(aes(x = time, y = pc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:11,
    minor_breaks = NULL,
    labels = c(
                "C", "C#|Db", "D", "D#|Eb",
                "E", "F", "F#|Gb", "G",
                "G#|Ab", "A", "A#|Bb", "B"
              )
  ) +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title = "Chromagram of a Randomly Sampled Human Track", x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()   # your second chromagram

grid.arrange(p1, p2, ncol = 2)

```

A chromagram shows how pitch classes (notes like C, D#, F, etc.) are used over time in a track. The vertical axis represents pitch classes, and the horizontal axis is time. Brighter colors indicate stronger presence of a pitch at a given moment. Above and below this text, you can see chromagrams from my two tracks and from two random tracks from the corpus (filtered on dancemusic). 

## AI Chromagrams
```{r, fig.width=14, fig.height=5}
library(tidyverse)
source("compmus_FV.R")
library(rjson)
library(gridExtra)

p1 <- "features/hidde-s-2.json" |>                           # Change the track
  compmus_chroma(norm = "manhattan") |>                 # Change the norm
  ggplot(aes(x = time, y = pc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:11,
    minor_breaks = NULL,
    labels = c(
                "C", "C#|Db", "D", "D#|Eb",
                "E", "F", "F#|Gb", "G",
                "G#|Ab", "A", "A#|Bb", "B"
              )
  ) +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title = "Chromagram of My AI Generated Track", x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()   
p2 <- "features/berend-b-2.json" |>                           # Change the track
  compmus_chroma(norm = "manhattan") |>                 # Change the norm
  ggplot(aes(x = time, y = pc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:11,
    minor_breaks = NULL,
    labels = c(
                "C", "C#|Db", "D", "D#|Eb",
                "E", "F", "F#|Gb", "G",
                "G#|Ab", "A", "A#|Bb", "B"
              )
  ) +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title = "Chromagram of a randomly sampled AI generated Track", x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()  

grid.arrange(p1, p2, ncol = 2)
```

In the human chromagrams we can see that there's more dynamic variation in the colours, we can see that colours are fading more often which could suggest chord changes or melodic motion. the vertical lines are less uniform than in the AI chromagrams indicating that melodies and harmonies are more varied and nuanced.

In the AI chromagrams color patterns appear more homogenous and repetitive. In both AI examples, there's also less contrast between sections—suggesting limited harmonic development or overuse of certain pitches and the melodies seem to hover around fewer notes, potentially leading to less emotional and musical variation.

Based on these visual cues, it does appear that AI-generated music—at least in these examples—produces less complex and varied melodic structures than human-composed tracks which could be due to a tendency of AI models to optimize for genre consistency rather than creativity and limitations in AI training data that may emphasize safe or average melodic choices. 

# 4. Chordprogressions between AI and Human made music is indistinguishable

##

Chordograms are visual representations that show the presence or intensity of different chord templates over time. They are generated by comparing chroma features (essentially pitch class distributions) from audio tracks to predefined chord templates using a distance metric (e.g., cosine or Euclidean). In essence, the lower the distance (darker colors), the better the match between the music at that time and a given chord template.

## Human Made

```{r chordogram Human, fig.width=14, fig.height=5}
library(tidyverse)
source("compmus_FV.R")

major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

p1 <- "features/hidde-s-1.json" |> 
  compmus_chroma(norm = "manhattan") |> 
  compmus_match_pitch_templates(
    chord_templates,         # Change to chord_templates if desired
    norm = "manhattan",       # Try different norms (and match it with what you used in `compmus_chroma`)
    distance = "manhattan"   # Try different distance metrics
  ) |>
  ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title = "Chordogram of my own human made track", x = "Time (s)", y = "Template", fill = NULL) +
  theme_classic() 

p2 <- "features/evan-l-2.json" |> 
  compmus_chroma(norm = "manhattan") |> 
  compmus_match_pitch_templates(
    chord_templates,         # Change to chord_templates if desired
    norm = "manhattan",       # Try different norms (and match it with what you used in `compmus_chroma`)
    distance = "manhattan"   # Try different distance metrics
  ) |>
  ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title = "Chordogram of a randomly sampled human made track", x = "Time (s)", y = "Template", fill = NULL) +
  theme_classic() 

grid.arrange(p1, p2, ncol = 2)
```

In the chordograms shown here, we compare human-composed and AI-generated dance tracks. Visually, these graphs are quite dense and noisy—showing rapid and complex fluctuations across many possible chord templates.

## AI GENERATED

```{r chordogram AI, fig.width=14, fig.height=5}
library(tidyverse)
source("compmus_FV.R")

major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

p1 <- "features/hidde-s-2.json" |> 
  compmus_chroma(norm = "manhattan") |> 
  compmus_match_pitch_templates(
    chord_templates,         # Change to chord_templates if desired
    norm = "manhattan",       # Try different norms (and match it with what you used in `compmus_chroma`)
    distance = "manhattan"   # Try different distance metrics
  ) |>
  ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title = "Chordogram of my own AI generated track", x = "Time (s)", y = "Template", fill = NULL) +
  theme_classic() 

p2 <- "features/berend-b-2.json" |> 
  compmus_chroma(norm = "manhattan") |> 
  compmus_match_pitch_templates(
    chord_templates,         # Change to chord_templates if desired
    norm = "manhattan",       # Try different norms (and match it with what you used in `compmus_chroma`)
    distance = "manhattan"   # Try different distance metrics
  ) |>
  ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title = "Chordogram of a randomly sampled AI generated track", x = "Time (s)", y = "Template", fill = NULL) +
  theme_classic() 

grid.arrange(p1, p2, ncol = 2)

```

Despite the intention to analyze harmonic clarity or predictability between human and AI music, these chordograms are too noisy to draw clear or meaningful conclusions. The abundance of activity across almost all chord templates suggests that the tracks—both human and AI—have rich harmonic content, or that the method used is overly sensitive. The only conclusion we can make is that there's no obvious difference between the two. 

# 5. Tempo is an uninteresting factor

## energy novelty:

An energy novelty function detects sudden changes in the energy of a track over time, which often corresponds to musical events like beat drops, new sections, or dramatic transitions.

In this comparison, we can see that the human-made track (left) has sharper and more defined peaks, suggesting more distinct transitions and dynamic shifts. In contrast, the AI-generated track (right) shows a more even distribution of energy changes, lacking distinct peaks. This could imply that the AI song has less structural variation or is less dynamic overall. This corresponds two how both songs sound as my own song has major shifts in volume while the AI song doesn't seem to do so. 

While both tracks have fluctuations, my human track appears to exhibit more contrast and intentional build-ups, which might contribute to a more engaging listening experience. 

## energy novelty function
```{r novelty, fig.width=14, fig.height=5}
source("compmus_FV.R")

p1 <- "features/Hidde-s-1.json" |>
  compmus_energy_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(title = "energy novelty function of my own human made track", x = "Time (s)", y = "Energy Novelty")

p2 <- "features/Hidde-s-2.json" |>
  compmus_energy_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(title = "energy novelty function of my AI generated track", x = "Time (s)", y = "Energy Novelty")

grid.arrange(p1, p2, ncol = 2)
```

##

Spectral novelty functions capture sudden changes in the frequency content of a track—essentially tracking how the timbre or instrumentation shifts over time.

As seen in the graphs below, both the human-made and AI-generated tracks display very noisy novelty curves, with dense and frequent peaks throughout. This noisiness is typical for dance music, which often features steady rhythmic patterns driven by drum machines and consistent percussive elements.

Because of this, the novelty functions don’t provide clear structural segmentation or meaningful differences between the two types of tracks. In this case, the spectral novelty function isn’t very useful for distinguishing between AI and human-generated music. 

## spectral novelty function
```{r spectral novelty, fig.width=14, fig.height=5}
source("compmus_FV.R")

p1 <- "features/Hidde-s-1.json" |>
  compmus_spectral_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(title = "spectral novelty function of my own human made track", x = "Time (s)", y = "Energy Novelty")

p2 <- "features/Hidde-s-2.json" |>
  compmus_spectral_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(title = "spectral novelty function of my AI generated track", x = "Time (s)", y = "Energy Novelty")

grid.arrange(p1, p2, ncol = 2)
```

##

Tempograms visualize the tempo of a track over time by analyzing periods in the energy signal. As you can see there isn't a difference or anything interesting to see since dance music is made with a drum machine and thus the tempo will remain steady as shown in these graphs wheter it is made by AI or not. 

## tempograms
```{r tempogram, fig.width=14, fig.height=5}
source("compmus_FV.R")

p1 <- "features/hidde-s-1.json" |>
  compmus_tempogram(window_size = 10, hop_size = 2, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(title = "tempogram of my own human made track", x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

p2 <- "features/hidde-s-2.json" |>
  compmus_tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(title = "tempogram of my AI generated track", x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

grid.arrange(p1, p2, ncol = 2)
```


# 6. AI generated music is structured more rigidly

## cepstograms

```{r cepstograms, fig.width=14, fig.height=5}
library(tidyverse)
source("compmus_FV.R")
library(rjson)
library(gridExtra)

p1 <- "features/hidde-s-1.json" |>                           # Change the track
  compmus_mfccs(norm = "manhattan") |>                  # Change the norm
  ggplot(aes(x = time, y = mfcc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:12,
    minor_breaks = NULL,
  ) +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title= "cepstogram of my human made track", x = "Time (s)", y = "Coefficient Number", fill = NULL) +
  theme_classic()   

p2 <- "features/hidde-s-2.json" |>                           # Change the track
  compmus_mfccs(norm = "manhattan") |>                  # Change the norm
  ggplot(aes(x = time, y = mfcc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:12,
    minor_breaks = NULL,
  ) +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(title = "Cepstrogram of my ai generated track", x = "Time (s)", y = "Coefficient Number", fill = NULL) +
  theme_classic()  

grid.arrange(p1, p2, ncol = 2)
```

##

These are cepstrograms, which are visual representations of cepstral coefficients (commonly MFCCs—Mel-Frequency Cepstral Coefficients) over time. These coefficients reflect the timbre or tone quality of audio. There's subtle but visible variation in the mid-to-lower cepstral coefficients in my human made track (especially 2–6), suggesting more nuanced changes in timbre throughout the track while the AI track is much more uniform and static across time, especially in higher-order coefficients. These cepstrograms hint that my human made track may explore more textural or timbral diversity, whereas the AI-generated track appears more repetitive or flat in this area. The differences are subtle and definitely not conclusive but they do this support a hypothesis that AI-generated tracks may be less timbrally adventurous or expressive than human ones—at least in this example.

## structuring

```{r SSMs, fig.width=14, fig.height=5}
  library(tidyverse)
  source("compmus_FV.R")
  library(rjson)
  
p1 <- "features/hidde-s-1.json" |>                           # Change the track
    compmus_chroma(norm = "manhattan") |>                 # Change the norm
    compmus_self_similarity(
      feature = pc,
      distance = "aitchison"                             # Change the distance
    ) |>   
    ggplot(aes(x = xtime, y = ytime, fill = d)) + 
    geom_raster() +
    scale_fill_viridis_c(guide = "none") +               # Change the colours?
    labs(x = "Time (s)", y = NULL, fill = NULL) +
    theme_classic()                                      # Change the theme?
  
p2 <- "features/hidde-s-2.json" |>                           # Change the track
  compmus_mfccs(norm = "euclidean") |>                  # Change the norm
  compmus_self_similarity(
    feature = mfcc,
    distance = "angular"                             # Change the distance
  ) |>   
  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +               # Change the colours?
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic() 
  
grid.arrange(p1, p2, ncol = 2)
```

##

The matrix corresponding to the human track is highly dense with a fine-grained texture. Diagonal stripes and criss-crossing bands suggest recurring patterns and sections—likely verse/chorus structure, breakdowns, or returning motifs. The AI tracks SSM is much more grid-like and rigid. Self-similarity reveals an important contrast:

The human made track shows more organic complexity and varied repetition while the AI generated track tends to be more predictable, possibly relying on pre-learned looping templates or structural repetition.

# 7. can we classify AI made songs? 

##

```{r classification knn, fig.width=14, fig.height=5}
library(tidyverse)
library(tidymodels)
library(ggdendro)
library(heatmaply)
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  

source("compmus_FV.R")

compmus2025_filtered <- 
  compmus_dance |> filter(!is.na(ai)) |> 
  mutate(ai = factor(if_else(ai, "AI", "Non-AI")))

compmus_cv <- compmus2025_filtered |> vfold_cv(5)

classification_recipe <-
  recipe(
    ai ~
      arousal +
      danceability +
      instrumentalness +
      tempo +
      valence,
    data = compmus2025_filtered
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].

knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")
classification_knn <- 
  workflow() |> 
  add_recipe(classification_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(compmus_cv, control = control_resamples(save_pred = TRUE))

classification_knn |> 
  get_conf_mat() |> 
  autoplot(type = "heatmap") +
  labs(title = "Confusion Matrix for KNN Classifier")

classification_knn |> get_pr()
```

##

The classifier struggles to clearly distinguish between AI and non-AI music as it misclassifies AI music as human more often (8 times) than it gets it right (6 times). It also frequently confuses human tracks as AI (6 false positives). The model performs slightly better on human music (with 9 correct predictions), but not reliably so.

## 

```{r classification RF, fig.width=14, fig.height=5}
library(tidyverse)
library(tidymodels)
library(ggdendro)
library(heatmaply)
source("compmus_FV.R")
library(gridExtra)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  

compmus2025_filtered <- 
  compmus_dance |> filter(!is.na(ai)) |> 
  mutate(ai = factor(if_else(ai, "AI", "Non-AI")))

compmus_cv <- compmus2025_filtered |> vfold_cv(5)

forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")

indie_forest <- 
  workflow() |> 
  add_recipe(classification_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    compmus_cv, 
    control = control_resamples(save_pred = TRUE)
  )
  
  workflow() |> 
  add_recipe(classification_recipe) |> 
  add_model(forest_model) |> 
  fit(compmus2025_filtered) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(title = "Randomforest classification", x = NULL, y = "Importance")

```

##

The bar graph above is the result of a randomforest classifier. Each bar represents how much that feature contributes to the classification decision. The longer the bar, the more influential that feature was in separating AI vs. human tracks. This tells us that while tempo doesn’t reveal much, how danceable or emotionally charged a track is might be subtly different between AI and human producers. The model is picking up on small stylistic clues — but the gap isn’t large enough for super-accurate classification (as we saw with the confusion matrix).

##

```{r}
indie_forest |> get_pr()
```

##

```{r RF graph}
compmus2025_filtered |>
  ggplot(aes(x = valence, y = arousal, colour = ai, size = tempo)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Tempo",
    y = "Danceability",
    size = "Tempo",
    colour = "AI"
  )
```

##

Danceability is slightly higher for AI tracks: The darker purple dots (AI) tend to cluster in the upper-right, suggesting that AI-generated tracks may favor higher danceability — possibly aiming for listener engagement. There's no clear tempo separation between AI and human tracks and both AI and non-AI dots appear at all bubble sizes, reaffirming what we’ve seen earlier: tempo isn’t a strong classifier.
However, human tracks do show more dispersion: Yellow (Non-AI) dots appear across a wider range of danceability values, from low to high while AI tracks are more tightly grouped, which might suggest a more uniform production style. We can thus conclude that AI-generated tracks tend to prioritize danceability and sit comfortably in a narrow tempo range — likely due to training on existing dance music datasets. Human producers, on the other hand, explore a slightly broader spectrum, especially on the lower end of danceability.

This supports the idea that AI might optimize for engagement, but not necessarily for variety or experimentation.

# 8. AI generated music is just as engaging, but less interesting as humanly produced music

After exploring a wide range of computational musicology tools — including chromagrams, chordograms, novelty functions, tempo analysis, cepstrograms, self-similarity matrices, and classification models — we arrive at a nuanced picture. AI-generated tracks are becoming increasingly difficult to distinguish from human-made ones — especially when evaluated through traditional audio features. However, subtle differences in structure, timbre, and expressive variation still give human compositions a slight edge in creativity and depth.

AI music isn't necessarily less “good,” but it may be less “interesting.”
It optimizes for what it knows — often danceability and repetition — but may lack the unpredictability, nuance, and emotional arcs of human creativity. 

-- Hidde Schuuring